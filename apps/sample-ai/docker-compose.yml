services:
  inference-api:
    build:
      context: ./inference-api
      dockerfile: ./Dockerfile
    environment:
      FLASK_ENV: docker
      COMMAND: app
      PORT: 8000
      RUN_ON_GPU: 0
      ALLOWED_EXTENSIONS: png,jpg,jpeg
      MODELS_FOLDER: /app/data/models
      UPLOAD_FOLDER: /app/data/uploads
      OUTPUT_FOLDER: /app/data/outputs
    ports:
      - 8000:8000
    networks:
      - sample-ai-app-network
    volumes:
      - /mnt/d/volumes/sample-ai-backend/:/app/data/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  frontend:
    build:
      context: ./frontend
      dockerfile: ./Dockerfile
      args:
        BUILD_ENV: docker
    ports:
      - 8080:80
    networks:
      - sample-ai-app-network

networks:
  sample-ai-app-network:
    driver: bridge
